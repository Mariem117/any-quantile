{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b7c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33357cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n",
      "/opt/conda/lib/python3.10/site-packages/statsforecast/utils.py:231: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  \"ds\": pd.date_range(start=\"1949-01-01\", periods=len(AirPassengers), freq=\"M\"),\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from datasetsforecast.long_horizon import LongHorizon\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "import os, pathlib\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from dataset import LongHorizonUnivariateDataModule, LongHorizonUnivariateDataset\n",
    "from dataset import ElectricityUnivariateDataModule, ElectricityUnivariateDataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "from utils.model_factory import instantiate\n",
    "\n",
    "from statsforecast.models import AutoETS, ETS, Theta, AutoCES\n",
    "\n",
    "from metrics import SMAPE, MAPE, CRPS\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='7'\n",
    "\n",
    "RESULTS_DIR = './results'\n",
    "from metrics import SMAPE, MAPE, CRPS\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed4ec9-947a-49d3-b461-60e4700031ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ckpt in tqdm(model_list):\n",
    "#     print(ckpt)\n",
    "#     model = torch.load(ckpt, map_location='cpu')\n",
    "#     model['hyper_parameters']['cfg']['model']['nn']['backbone']['_target_'] = 'modules.AqOutTransformer'\n",
    "#     torch.save(model, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2346e506-e750-4d58-8e8f-25da18952733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Checking data directory structure:\n",
      "   ./data/ exists\n",
      "      downloads\n",
      "      electricity\n",
      "      emhires\n",
      "\n",
      "   ./data/electricity/ contents:\n",
      "      datasets\n",
      "\n",
      "   ./data/electricity/datasets/ contents:\n",
      "         .ipynb_checkpoints\n",
      "\n",
      "2. Searching for any CSV files in data directory:\n",
      "   Found 0 CSV files:\n",
      "\n",
      "3. Searching for df_y.csv files:\n",
      "   Found 0 df_y.csv files:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Check if data directory exists\n",
    "print(\"1. Checking data directory structure:\")\n",
    "if os.path.exists('./data/'):\n",
    "    print(\"   ./data/ exists\")\n",
    "    for item in os.listdir('./data/'):\n",
    "        print(f\"      {item}\")\n",
    "        \n",
    "    # Check electricity subdirectory\n",
    "    if os.path.exists('./data/electricity/'):\n",
    "        print(\"\\n   ./data/electricity/ contents:\")\n",
    "        for item in os.listdir('./data/electricity/'):\n",
    "            print(f\"      {item}\")\n",
    "            \n",
    "        # Check datasets subdirectory\n",
    "        if os.path.exists('./data/electricity/datasets/'):\n",
    "            print(\"\\n   ./data/electricity/datasets/ contents:\")\n",
    "            for item in os.listdir('./data/electricity/datasets/'):\n",
    "                print(f\"         {item}\")\n",
    "else:\n",
    "    print(\"   ./data/ does not exist!\")\n",
    "\n",
    "print(\"\\n2. Searching for any CSV files in data directory:\")\n",
    "csv_files = glob(\"./data/**/*.csv\", recursive=True)\n",
    "print(f\"   Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files[:20]:\n",
    "    print(f\"   {f}\")\n",
    "\n",
    "print(\"\\n3. Searching for df_y.csv files:\")\n",
    "df_y_files = glob(\"**/df_y.csv\", recursive=True)\n",
    "print(f\"   Found {len(df_y_files)} df_y.csv files:\")\n",
    "for f in df_y_files:\n",
    "    print(f\"   {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a83b7-f8aa-4cbf-8536-07894cc423ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"model-epoch=14.ckpt\"\n",
    "\n",
    "# Change these to match your actual checkpoints\n",
    "backbone = \"modules.NBEATSAQCAT\"  # Changed from NBEATSAQFILM\n",
    "maxnorm = True  # Changed from False\n",
    "\n",
    "# Rest of your parameters\n",
    "blocks = 30\n",
    "lr = 0.0005\n",
    "width = 1024\n",
    "layers = 3\n",
    "warmup = 400\n",
    "train_q = 1\n",
    "quantile_embed_num = 100\n",
    "quantile_embed_dim = 64\n",
    "loss = \"MQNLoss\"\n",
    "seed = \"*\" \n",
    "\n",
    "# Fixed checkpoint pattern - removed 'test/' from path\n",
    "if 'NBEATS' in backbone:\n",
    "    checkpoint_pattern = f\"lightning_logs/MHLV/model=model.AnyQuantileForecaster-backbone={backbone}-history=168-lr={lr}-width={width}-layers={layers}-blocks={blocks}-warmup={warmup}-maxnorm={maxnorm}-loss=losses.{loss}-seed={seed}/checkpoints/{checkpoint_name}\"\n",
    "elif 'Cnn' in backbone:\n",
    "    if loss == \"MQNLoss\":\n",
    "        checkpoint_pattern = f\"lightning_logs/MHLV/model=model.GeneralAnyQuantileForecaster-backbone={backbone}-history=168-lr={lr}-width={width}-train_q={train_q}-quantile_embed_num={quantile_embed_num}-quantile_embed_dim={quantile_embed_dim}-maxnorm={maxnorm}-loss=losses.{loss}-seed={seed}/checkpoints/{checkpoint_name}\"\n",
    "    else:\n",
    "        checkpoint_pattern = f\"lightning_logs/MHLV/model=model.GeneralAnyQuantileForecaster-backbone={backbone}-history=168-lr={lr}-width={width}-train_q={train_q}-quantile_embed_num={quantile_embed_num}-quantile_embed_dim={quantile_embed_dim}-maxnorm={maxnorm}*-seed={seed}/checkpoints/{checkpoint_name}\"\n",
    "elif 'Transformer' in backbone:\n",
    "    checkpoint_pattern = f\"lightning_logs/MHLV/model=model.GeneralAnyQuantileForecaster-backbone={backbone}-history=168-lr={lr}-width={width}-blocks={blocks}-train_q={train_q}-quantile_embed_num={quantile_embed_num}-quantile_embed_dim={quantile_embed_dim}-maxnorm={maxnorm}-loss=losses.{loss}-seed={seed}/checkpoints/{checkpoint_name}\"\n",
    "\n",
    "model_list = glob(checkpoint_pattern)\n",
    "\n",
    "print(f\"Searching for: {checkpoint_pattern}\")\n",
    "print(f\"Found {len(model_list)} matches:\")\n",
    "for m in model_list:\n",
    "    print(f\"  {m}\")\n",
    "\n",
    "if len(model_list) == 0:\n",
    "    raise FileNotFoundError(f\"No checkpoints found matching pattern: {checkpoint_pattern}\")\n",
    "\n",
    "cfg = torch.load(model_list[0], map_location='cpu')['hyper_parameters']\n",
    "cfg = OmegaConf.create(cfg).cfg\n",
    "\n",
    "cfg.dataset.split_boundaries = ['2006-01-01', '2017-01-01', '2018-01-01', '2019-01-01']\n",
    "dm = instantiate(cfg.dataset)\n",
    "dm.setup(stage='test')\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for b in tqdm(test_loader):\n",
    "    df = pd.DataFrame.from_dict({k: list(v.cpu().numpy()) for k,v in b.items() if\n",
    "                                k in ['target', 'history', 'series_id', 'quantiles']})\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03755b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_random = 100\n",
    "num_deterministic = 101\n",
    "num_random = 100\n",
    "num_deterministic = 101\n",
    "\n",
    "# Validate total quantiles\n",
    "TOTAL_QUANTILES = num_random + num_deterministic\n",
    "print(f\"Expected total quantiles: {TOTAL_QUANTILES}\")\n",
    "print(f\"Actual quantiles per sample: {len(df.quantiles.iloc[0])}\")\n",
    "\n",
    "# Verify the split makes sense\n",
    "if len(df.quantiles.iloc[0]) != TOTAL_QUANTILES:\n",
    "    raise ValueError(\n",
    "        f\"Quantile count mismatch! Expected {TOTAL_QUANTILES}, \"\n",
    "        f\"got {len(df.quantiles.iloc[0])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44754fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_checkpoint(checkpoint_file, trainer):\n",
    "    cfg = torch.load(checkpoint_file)['hyper_parameters']\n",
    "    cfg = OmegaConf.create(cfg).cfg\n",
    "    \n",
    "    model = instantiate(cfg.model, cfg).load_from_checkpoint(checkpoint_file)\n",
    "\n",
    "    predictions = trainer.predict(model, dataloaders=dm.test_dataloader())\n",
    "    predictions = torch.cat(predictions).detach().cpu()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "trainer = pl.Trainer(accelerator='gpu', devices=1)\n",
    "predictions = []\n",
    "for ckpt in model_list:\n",
    "    pred = predict_checkpoint(ckpt, trainer)\n",
    "    predictions.append(pred)\n",
    "    \n",
    "#     predictions_deterministic = pred[...,mid_idx-num_deterministic//2:mid_idx+num_deterministic//2+1]\n",
    "#     predictions_random = torch.cat([pred[..., 0:mid_idx-num_deterministic//2], \n",
    "#                                     pred[..., mid_idx+num_deterministic//2+1:]], dim=-1)\n",
    "#     predictions_random, _ = torch.sort(predictions_random, dim=-1)\n",
    "#     predictions_sort = torch.cat([predictions_deterministic, predictions_random], dim=-1)\n",
    "    \n",
    "#     predictions.append(predictions_sort)\n",
    "    \n",
    "predictions_ensemble = torch.stack(predictions).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_rnd = CRPS()\n",
    "crps_fixed = CRPS()\n",
    "\n",
    "for target, pred, q in tqdm(zip(df.target, predictions_ensemble, df.quantiles), total=len(df)):\n",
    "    \n",
    "    if np.isinf(target).any() or np.isnan(target).any():\n",
    "        continue\n",
    "\n",
    "    # Fix: Extract the actual quantile and prediction arrays from the row\n",
    "    q = np.array(q)  # Convert to numpy array if it isn't already\n",
    "    pred = np.array(pred)  # Convert to numpy array if it isn't already\n",
    "    target = np.array(target)  # Convert to numpy array if it isn't already\n",
    "    \n",
    "    # Split quantiles and predictions (deterministic first, then random)\n",
    "    q_deterministic = q[:num_deterministic]\n",
    "    q_random = q[num_deterministic:]\n",
    "    \n",
    "    predictions_deterministic = pred[:num_deterministic]\n",
    "    predictions_random = pred[num_deterministic:]\n",
    "    \n",
    "    # Update metrics with proper array indexing\n",
    "    crps_rnd.update(\n",
    "        preds=torch.Tensor(predictions_random)[None], \n",
    "        target=torch.Tensor(target)[None], \n",
    "        q=torch.Tensor(q_random)[None]\n",
    "    )\n",
    "    \n",
    "    crps_fixed.update(\n",
    "        preds=torch.Tensor(predictions_deterministic)[None], \n",
    "        target=torch.Tensor(target)[None], \n",
    "        q=torch.Tensor(q_deterministic)[None]\n",
    "    )\n",
    "    \n",
    "print(\"CRPS random quants\", crps_rnd.compute().cpu().numpy())\n",
    "print(\"CRPS mandatory quants\", crps_fixed.compute().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9641c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = f'results/MHLV/{backbone.split(\".\")[-1]}-maxnorm={maxnorm}-loss={loss}'\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "for worker in tqdm(range(len(predictions))):\n",
    "    p = predictions[worker]\n",
    "    for series_id in df.series_id.unique():\n",
    "        series_mask = df.series_id == series_id\n",
    "\n",
    "        df_series = df[series_mask]\n",
    "        p_series = p[series_mask.values].numpy()\n",
    "        \n",
    "        target_series = np.array([v for v in df.target[series_mask.values]])\n",
    "        target_series = np.nan_to_num(target_series, posinf=np.nan)\n",
    "        target_series = np.repeat(target_series[...,None], p.shape[-1], axis=-1)\n",
    "        \n",
    "        quantile_series = np.array([v for v in df.quantiles[series_mask.values]])\n",
    "        quantile_series = np.repeat(quantile_series[:,None], p.shape[1], axis=1)\n",
    "        \n",
    "        forec = pd.DataFrame({f\"forec{worker+1}\": p_series.ravel()})\n",
    "        if worker == 0:\n",
    "            forec['actuals'] = target_series.ravel()\n",
    "            forec['quants'] = quantile_series.ravel()\n",
    "            forec = forec[['actuals', 'quants', 'forec1']]\n",
    "        \n",
    "        forec.to_pickle(os.path.join(RESULTS_PATH, f'e1w{worker+1}_{series_id}.pickle'))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab0591-bd99-424d-a8be-92af86aad267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
