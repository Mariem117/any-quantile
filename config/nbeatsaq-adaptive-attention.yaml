logging:
  path: lightning_logs
  name: nbeatsaq-final

dataset:
  _target_: dataset.ElectricityUnivariateDataModule
  name: MHLV
  train_batch_size: 512
  eval_batch_size: 512
  num_workers: 4
  persistent_workers: true
  split_boundaries:
    - "2006-01-01"
    - "2017-01-01"
    - "2018-01-01"
    - "2019-01-01"
  history_length: 168
  horizon_length: 48
  fillna: "ffill"
  train_step: 12
  eval_step: 24

random:
  seed: [0, 1, 2, 3, 4, 5, 6, 7]

trainer:
  max_epochs: 20
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  devices: 1
  accelerator: gpu
  precision: 32
  gradient_clip_val: 1.0
  accumulate_grad_batches: 2

checkpoint:
  save_top_k: 3
  ckpt_path: null
  resume_ckpt: null

model:
  _target_: model.AnyQuantileForecasterAdaptiveAttention

  nn:
    backbone:
      _target_: modules.NBEATSAQCAT
      num_blocks: 20
      num_layers: 3
      layer_width: 832
      share: false
      size_in: ${dataset.history_length}
      size_out: ${dataset.horizon_length}
      dropout: 0.03
      quantile_embed_dim: 64
      quantile_embed_num: 100

  input_horizon_len: ${dataset.history_length}
  loss:
    _target_: losses.MQNLoss
  q_sampling: random_in_batch
  q_distribution: uniform
  q_parameter: 0.25
  max_norm: true

  adaptive_sampling:
    num_adaptive_quantiles: 7
    num_bins: 100
    momentum: 0.99
    temperature: 1.0
    min_prob: 0.001

  adaptive_attention:
    d_model: 256
    n_heads: 8
    dropout: 0.03
    d_ff: 1024
    adaptive_temp: 1.0
    num_blocks: 2

  optimizer:
    _target_: torch.optim.Adam
    lr: 0.0005
    weight_decay: 0.00001

  scheduler:
    _target_: schedulers.InverseSquareRoot
    warmup_updates: 800
    warmup_end_lr: 0.0005
