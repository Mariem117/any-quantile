# FIXED Monotonicity Config
# Note: Monotonicity is likely redundant since MQNLoss already encourages it
# Expected result: Similar or slightly worse than base (CRPS ~58-65)

logging:
  path: lightning_logs
  name: nbeatsaq-monotone-seed=${random.seed}

dataset:
  _target_: dataset.ElectricityUnivariateDataModule # CRITICAL: Fixed class name!
  name: MHLV
  train_batch_size: 256 # CRITICAL: Changed from 512
  eval_batch_size: 512
  num_workers: 8 # Increased from 4
  persistent_workers: true # Changed from false
  split_boundaries: # CRITICAL: Fixed boundaries!
    - "2006-01-01"
    - "2017-01-01"
    - "2018-01-01"
    - "2019-01-01"
  history_length: 168
  horizon_length: 48 # CRITICAL: Changed from 24!
  fillna: "ffill"
  train_step: 12 # CRITICAL: Changed from 1!
  eval_step: 24

random:
  seed: [6]

trainer:
  max_epochs: 15
  check_val_every_n_epoch: 1
  log_every_n_steps: 50 # Changed from 100
  devices: 1
  accelerator: gpu
  precision: 32
  gradient_clip_val: 0.5
  accumulate_grad_batches: 1 # Added

checkpoint:
  save_top_k: 3 # Increased from 2
  monitor: val/crps # CRITICAL: Added monitoring!
  mode: min
  ckpt_path: null
  resume_ckpt: null

model:
  _target_: model.AnyQuantileForecasterWithMonotonicity

  input_horizon_len: ${dataset.history_length}

  loss:
    _target_: losses.MQNLoss

  q_sampling: random_in_batch
  q_distribution: beta # CRITICAL: Changed from uniform!
  q_parameter: 0.3
  max_norm: true

  # Monotonicity parameters
  monotone_weight: 0.1 # Start moderate
  monotone_margin: 0.01
  num_train_quantiles: 9

  nn:
    backbone:
      _target_: modules.NBEATSAQCAT
      num_blocks: 24 # Changed from 30 (more reasonable)
      num_layers: 4 # Increased from 3
      layer_width: 1024
      share: false
      size_in: ${dataset.history_length}
      size_out: ${dataset.horizon_length}
      dropout: 0.05 # CRITICAL: Changed from 0.0!
      quantile_embed_dim: 128 # Increased from 64
      quantile_embed_num: 100

  optimizer:
    _target_: torch.optim.AdamW # CRITICAL: Changed from Adam!
    lr: 0.001 # Increased from 0.0001
    weight_decay: 0.0001 # Increased from 0.00001
    betas: [0.9, 0.999]

  scheduler:
    _target_: schedulers.InverseSquareRoot
    warmup_updates: 1000 # Increased from 400
    warmup_end_lr: 0.001 # Match lr


# WARNING: Monotonicity is likely redundant!
# - MQNLoss (pinball loss) already encourages monotonic quantiles
# - Expected performance: CRPS ~58-65 (similar or slightly worse than base)
# - Main benefit: Explicit guarantee of no crossing quantiles
#
# Your base model (no monotonicity) already achieved CRPS 56.71
# This will likely not improve much, if at all
