#!/usr/bin/env python3
"""
Simple test script to validate configuration without requiring all dependencies.
"""

from omegaconf import OmegaConf

def test_config():
    """Test the adaptive attention configuration."""
    print("ğŸ” Testing Adaptive Attention Configuration")
    print("=" * 50)
    
    # Load configuration
    cfg = OmegaConf.load('config/nbeatsaq-adaptive-attention-mhlv.yaml')
    
    # Basic validation
    print("âœ… Configuration loaded successfully")
    print(f"ğŸ“‹ Model target: {cfg.model._target_}")
    print(f"ğŸ“‹ Dataset: {cfg.dataset.name}")
    print(f"ğŸ“‹ History length: {cfg.dataset.history_length}")
    print(f"ğŸ“‹ Horizon length: {cfg.dataset.horizon_length}")
    
    # Adaptive sampling config
    if hasattr(cfg.model, 'adaptive_sampling'):
        print("\nğŸ¯ Adaptive Sampling Configuration:")
        sampling_cfg = cfg.model.adaptive_sampling
        print(f"   â€¢ Adaptive quantiles: {sampling_cfg.num_adaptive_quantiles}")
        print(f"   â€¢ Number of bins: {sampling_cfg.num_bins}")
        print(f"   â€¢ Momentum: {sampling_cfg.momentum}")
        print(f"   â€¢ Temperature: {sampling_cfg.temperature}")
        print(f"   â€¢ Min probability: {sampling_cfg.min_prob}")
    
    # Adaptive attention config
    if hasattr(cfg.model, 'adaptive_attention'):
        print("\nğŸ§  Adaptive Attention Configuration:")
        attention_cfg = cfg.model.adaptive_attention
        print(f"   â€¢ Model dimension: {attention_cfg.d_model}")
        print(f"   â€¢ Number of heads: {attention_cfg.n_heads}")
        print(f"   â€¢ Dropout: {attention_cfg.dropout}")
        print(f"   â€¢ Feed-forward dim: {attention_cfg.d_ff}")
        print(f"   â€¢ Adaptive temperature: {attention_cfg.adaptive_temp}")
        print(f"   â€¢ Number of blocks: {attention_cfg.num_blocks}")
    
    # Training config
    print("\nğŸ‹ï¸ Training Configuration:")
    print(f"   â€¢ Max epochs: {cfg.trainer.max_epochs}")
    print(f"   â€¢ Batch size: {cfg.dataset.train_batch_size}")
    print(f"   â€¢ Learning rate: {cfg.model.optimizer.lr}")
    print(f"   â€¢ Gradient clipping: {cfg.trainer.gradient_clip_val}")
    print(f"   â€¢ Warmup updates: {cfg.model.scheduler.warmup_updates}")
    
    # Backbone config
    print("\nğŸ—ï¸ Backbone Configuration:")
    backbone_cfg = cfg.model.nn.backbone
    print(f"   â€¢ Type: {backbone_cfg._target_}")
    print(f"   â€¢ Number of blocks: {backbone_cfg.num_blocks}")
    print(f"   â€¢ Layer width: {backbone_cfg.layer_width}")
    print(f"   â€¢ Number of layers: {backbone_cfg.num_layers}")
    print(f"   â€¢ Dropout: {backbone_cfg.dropout}")
    
    print("\nâœ… Configuration validation complete!")
    print("\nğŸ“– Usage:")
    print("   python train_adaptive_attention.py --config config/nbeatsaq-adaptive-attention-mhlv.yaml")
    print("   python train_adaptive_attention.py --config config/nbeatsaq-adaptive-attention-mhlv.yaml --fast-dev-run")

if __name__ == "__main__":
    test_config()
